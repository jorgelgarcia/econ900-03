%Input preamble
\input{preamble}
\let\counterwithout\relax
\let\counterwithin\relax
\definecolor{maroon}{HTML}{4B0082}

\begin{document}

\noindent \textbf{900-03: Econometrics II: Advanced Regression Analysis and Empirical Design}\\
\noindent Instructor: Jorge Luis Garc√≠a \\
e-mail: jlgarci@clemson.edu\\

\noindent \textbf{Problem Set 4.}\\

\noindent \textbf{Submission instructions for theory part (Problems 1 to 6):} Place a pdf with your answer in the Box folder econ900/econ900-03/problemset4. \textit{Do not me e-mail your pdf}.\\

\noindent \textbf{Submission instructions for empirical part (Problem 7):} Push the code for your empirical problem separately with the name problem``numberofproblem"\_``yourlastname".do. Do so to a folder in the repository called problemset4 (you need to create that folder in your local). \textit{Do not e-mail me your code}.\\

\noindent \textbf{Due date:} 11/22/2020 at 2:00 p.m.\\

\noindent \textbf{Problem 0 (A Blast from the Past). Grouped Regression.}\\

\noindent Consider the linear model 
\begin{equation} 
y_{it} = \alpha_0 + \alpha_1 x_{it} + \varepsilon_{it}, 
\end{equation}
\noindent where the usual notation applies and $x_{it} \independent \varepsilon_{it}$. Note that $x_{it}$ is a scalar.  Suppose that $x_{it} = W_{kt}$, where $k$ indexes a ``group.'' That is,  $x_{it}$ does not vary at the individual $i$ level; instead, it varies at the group-$k$ level. Show that the OLS estimate of $\alpha_1$ is identical to the WLS estimate of the following ``grouped'' regression: 
\begin{equation} 
Y_{kt} = \alpha_0 + \alpha_1 W_{kt} + \varepsilon_{kt}, 
\end{equation}
\noindent where $Y_{kt}$ is the group-$k$ average of $y_{it}$ and the weight for group $k$ is the number of observations in the $k$ group, $n_{k}$.\\
\noindent \textbf{Answer:}  First, dropping the intercept when $x_{it}$ is categorical changes the interpretation of the coefficients but not the informational content of the OLS estimator. Second, time is irrelevant in this context. There is no time fixed effects or something similar to that. Thus, I drop the constant and time indices.\\

\noindent Let $\mathcal{I}_{k}$ be the index set for individuals in group $k = 1, \ldots, K$. The cardinality of this set is $n_k$. The OLS estimator details when considering the full-sample regression are

\begin{eqnarray} 
\left( X' X \right) &=& {\left( \left[ w_1, \ldots, w_1, \ldots, w_K, \ldots, w_K \right] \cdot \left[w_1, \ldots, w_1, \ldots, w_K, \ldots, w_K \right]' \right)}^{-1} \nonumber \\ 
                         &=& \frac{1}{\sum \limits _{\mathcal{I}_1} w_1^2 + \cdots + \sum \limits _{\mathcal{I}_K} w_K^2} \nonumber \\
                         &=& n_1 w_1^2 + \cdots + n_K w_K^2
\end{eqnarray}

\noindent and

\begin{eqnarray} 
\left( X' Y \right) &=& {\left( \left[w_1, \ldots, w_1, \ldots, w_K, \ldots, w_K \right] \cdot \left[ y_1, \ldots, y_n \right]' \right)}^{-1} \nonumber \\ 
                         &=& \sum \limits _{\mathcal{I}_1} w_1 y_1 + \cdots + \sum \limits _{\mathcal{I}_K} w_K y_K \nonumber \\ 
                         &=&  w_1  \cdot \sum \limits _{\mathcal{I}_1} y_1 + \cdots + w_K \cdot \sum \limits _{\mathcal{I}_K} y_K.
\end{eqnarray}

\noindent Define $\bar{y}_k := \frac{1}{n_k} \sum \limits _{I_{k}} y_{i}$ (i.e.,\ the average outcome in group $k$). The WLS estimator details when considering the grouped weighted regression are: 

\begin{eqnarray} 
\left( \tilde{X}' W \tilde{X} \right) &=&  {\left( \left[ w_1, \ldots, w_K \right] \cdot  \text{diag} \left( \left[ n_1, \ldots, n_K \right] \right) \cdot  \left[ w_1, \ldots, w_K \right]' \right)}^{-1} \nonumber \\
                         &=& n_1 w_1^2 + \cdots + n_K w_K^2
\end{eqnarray}

\noindent and

\begin{eqnarray} 
\left( \tilde{X}' W \tilde{Y} \right) &=&  {\left( \left[ w_1, \ldots, w_K \right] \cdot  \text{diag} \left( \left[ n_1, \ldots, n_K \right] \right) \cdot \left[\bar{y}_1, \ldots, \bar{y}_K \right]' \right)} \nonumber \\ 
			&=& w_1  n_1  \frac{1}{n_1} \sum \limits _{I_{1}} y_{i} + \cdots + w_K n_K  \frac{1}{n_K} \sum \limits _{I_{K}} y_{i} \nonumber \\ 
                         &=&  w_1  \cdot \sum \limits _{\mathcal{I}_1} y_1 + \cdots + w_K \cdot \sum \limits _{\mathcal{I}_K} y_K.
\end{eqnarray}
 
\noindent Because $\left( X' X \right) = \left( \tilde{X}' W \tilde{X} \right)$ and $\left( X' Y \right)  = \left( \tilde{X}' W \tilde{Y} \right)$, the OLS estimator in the full sample and the WLS estimator in the grouped sample lead to the same estimate of $\alpha_1$.\\

\noindent \textbf{Problem 1. The Differences in Difference-in-Differences.}\\
\begin{enumerate}
\item Enunciate the linear model allowing you to estimate the ATT using difference-in-differences in the baseline two-group, two-period framework.\\
\noindent \textbf{Answer:}
\begin{equation} 
y_{it} = \beta_0 + \beta_1 \cdot \bm{1} \left[ \text{after}_{t} \right] +  \beta_2 \cdot \bm{1} \left[ \text{treatment}_i \right] +  \beta_3 \cdot \bm{1} \left[ \text{after}_{t} \cdot \text{treatment}_i \right] + \varepsilon_{it}, 
\end{equation}
\noindent where $\bm{1} \left[ \cdot \right]$ is an indicator function and $\varepsilon_{it}$ an error term.
 
\item Suppose that the error term in your model is an individual-level lottery. Express each of the parameters of your model in terms of expectations and answer: What difference does each parameter identify?\\
\noindent \textbf{Answer:}
\begin{eqnarray} 
\text{Control, Before} &:& \mathbb{E} \left[ y_{it} |  \text{after}_t = \text{treatment}_i = 0 \right] = \beta_0 \nonumber \\
\text{Control, After} &:& \mathbb{E} \left[ y_{it} |  \text{after}_t =1, \text{treatment}_i = 0 \right] = \beta_0 + \beta_1 \nonumber \\ 
\text{Second Difference} &:& \beta_1 \nonumber \\
\text{Treatment, Before} &:& \mathbb{E} \left[ y_{it} |  \text{after}_t = 0, \text{treatment}_i = 1 \right] = \beta_0 + \beta_2 \nonumber \\
\text{Treatment, After} &:& \mathbb{E} \left[ y_{it} |  \text{after}_t =1, \text{treatment}_i = 1 \right] = \beta_0 + \beta_1 + \beta_2 + \beta_3 \nonumber \\ 
\text{First Difference} &:& \beta_1 + \beta_3 \nonumber \\
\text{Difference-in-differences} &:& \text{First Difference} - \text{Second Difference} = \beta_3.
\end{eqnarray} 
\end{enumerate}

\noindent \textbf{Problem 2. Difference-in-Difference-in-Differences.}\\
\begin{enumerate} 
\item Enunciate the linear model allowing you to estimate difference-in-difference-in-differences. Assume that there are two periods ($t$), two treatment statuses $d$, and two locations $l$. In one location, there is not treatment at all.\\
\noindent \textbf{Answer:} 
\begin{eqnarray} 
y_{itl} &=& \beta_0 + \beta_1 \cdot \bm{1} \left[ \text{after}_{t} \right] +  \beta_2 \cdot \bm{1} \left[ \text{treatment}_i \right] +  \beta_3 \cdot \bm{1} \left[ \text{location}_l \right] \nonumber \\ 
&+& \beta_4 \cdot \bm{1} \left[ \text{after}_{t} \cdot \text{treatment}_i \right] + \beta_5 \cdot \bm{1} \left[ \text{after}_{t} \cdot \text{location}_l \right] + \beta_6 \cdot \bm{1} \left[ \text{treatment}_{i} \cdot \text{location}_l \right] \nonumber \\ 
&+& \beta_7 \cdot \bm{1} \left[ \text{after}_{t}  \cdot \text{treatment}_{i} \cdot \text{location}_l \right]  + \varepsilon_{itl}, 
\end{eqnarray}

\noindent where the usual notation applies.

\item Suppose that the error term in your model is an individual-level lottery. Express each of the parameters of your model in terms of expectations and answer: What difference does each parameter identify and under what assumptions(s)?\\
\noindent \textbf{Answer:} The answer is analogous to \textbf{Problem 4}. The difference across the diff-in-diff in each location identifies the diff-in-diff-in-diff, $\beta_7$. 
\end{enumerate}

\noindent \textbf{Problem 3. Inference in Difference-in-Differences.}\\
\noindent Consider the model
\begin{equation} 
y_{ikt} = \beta D_{kt} + \lambda_k + \lambda_t + \varepsilon_{ikt}. \label{eq:diff2}
\end{equation}

\noindent Suppose that $\mathbb{E}\left[\varepsilon_{ikt}\varepsilon_{jkt}\right] \neq 0$ for $i \neq j$. A researcher proposes an inference procedure based on $\varepsilon_{ikt}:= \nu_{kt} + \tilde{\varepsilon}_{ikt}$ with $\tilde{\varepsilon}_{ikt}$ being assigned by a an individual-level lottery. 

\begin{enumerate}
\item Is the researcher's procedure based on a parametric assumption?\\
\noindent \textbf{Answer:} Yes. $\varepsilon_{ikt}:= \nu_{kt} + \tilde{\varepsilon}_{ikt}$ is a parameterization of the variance.
\item Propose an estimator for the variance of $\beta$ for the researcher.\\
\noindent \textbf{Answer:} Given the assumptions $\varepsilon_{ikt}:= \nu_{kt} + \tilde{\varepsilon}_{ikt}$, feasible generalized least squares is a straightforward option.
\item Give them a concrete alternative using a Huber-White variance estimator.\\
\noindent \textbf{Answer:} Let $X$ be the matrix containing all of the regressors and $\bm{\beta}$ all of the estimands. Recall that the Huber-White sandwich variance estimator is  
\begin{eqnarray} 
\var \left( \bm{\beta} \right) &=& \frac{1}{N} \left[ \frac{X'X}{N}  \left[ \Omega \left( \lambda \right)  \right]   \frac{X'X}{N} \right], 
\end{eqnarray} 

\noindent where the $kk'$ entry of  $\left[ \Omega \left( \lambda \right)  \right]$ is $\frac{\sum _{\mathcal{I}} \sum _{\mathcal{I}} \lambda \left( i, j \right) X_{ki} X_{k'j} \hat{\varepsilon}_{i} \hat{\varepsilon}_{j}  }{N}$, where $\mathcal{I}$ is the index set for the individuals in the sample and $\hat{\varepsilon}_{i}$ is the residual of Equation~\eqref{eq:diff2} for individual $i \in \mathcal{I}$. In this case, $\lambda \left( i, j \right) = 1$ if $i$ and $j$ belong to the same group $k$. Else, $\lambda \left( i, j \right) = 0$. This is usually referred to as the robust estimator of the variance clustered at the group-$k$ level. 

\item Explain to them if this relaxes their assumptions, if they are making any.\\
\noindent \textbf{Answer:} This relaxes their assumption because it allows for \textit{any} form of hetroskedasticity and cross-$k$ correlation, not necessarily that given by $\varepsilon_{ikt}:= \nu_{kt} + \tilde{\varepsilon}_{ikt}$.
\item Explain to them how to obtain a variance estimate using a block-$k$ bootstrap procedure.\\
\noindent \textbf{Answer:} Let $K$ be the number of groups and $\bm{\Theta}$ the vector containing the statistics of interest (e.g.,\ $\beta$). The researcher can resample $K$ groups with replacement to form $B$ samples. In each of these resamples they would calculate the statistic  $\bm{\Theta}_b$ for $b = 1, \ldots, B$. The standard deviation across resamples of each of their statistics are the standard errors.
\item Explain to them if this relaxes their assumptions, if they are making any.\\
\noindent \textbf{Answer:} Yes. Bootstrap does not assume a specific structure for $\varepsilon_{ikt}$ and allows for \textit{any} form of hetroskedasticity and cross-$k$ correlation
\item Would they be able to improve their inference if grouping their model at the $k$ level and then obtaining different variance estimators? \\
\noindent \textbf{Answer:} No. While grouping the model does lead to the same estimates, as shown in \textbf{Problem 1}, the weights of the grouping adjust the variance so that the inference does not change (you can show that, but note that this is desirable property. Else researchers would choose to either group or not their model based on their preferred inference results).
\end{enumerate} 

\noindent \textbf{Problem 4. The Average and Local Average Treatment Effects.} Let a linar model be 
\begin{eqnarray}
y_{i} &=& y_{i0} + D_{i} \cdot \left( y_{i1} - y_{i0} \right) \nonumber \\ 
        &=& \beta_0 + \beta_1 D_{i} + \varepsilon_{i}.  
\end{eqnarray}
\begin{enumerate} 
\item Use the material that we discussed in class to interpret each of the elements in this model.\\
\noindent \textbf{Answer:} The postulated model is of the switching form: $y_{i} = y_{i0} + D_{i} \cdot \left( y_{i1} - y_{i0} \right)$. An interpretation for it is: 
\begin{eqnarray} 
y_{i} &=& y_{i0} + D_{i} \cdot \left( y_{i1} - y_{i0} \right) \nonumber \\ 
&=& \mathbb{E} \left[ y_{i0} \right] + D_{i} \cdot \left( y_{i1} - y_{i0} \right) +  \left( y_{i0} -  \mathbb{E} \left[ y_{i0} \right] \right) \nonumber \\
&:=& \alpha + \beta \cdot D_{i} + \varepsilon_{i} , 
\end{eqnarray}
\noindent motivating that $\beta, D_{i}$ are not independent from $\varepsilon_i$.  

\item State the assumptions of the instrumental-variable setting for this model.\\ 
\noindent \textbf{Answer:} This comes straight from the notes. Use the notation above and consider, additionally, a binary instrument $z_i$. The assumptions are 
\begin{itemize}
\item \textbf{Random assignment of the Instrument:} $y_{iz}^d, D_{iz}  \independent z_{i}$, for any values $d,z$. That is, the assignment of $z_{i}$ is independent of the realizations of the outcome or treatment take up.
\item  \textbf{Exclusion Restriction:} $y_{i0}^d = y_{i1}^d$ for any value of $d$. That is, the instrument has no effect on $y_{i}$ on its own. Any effect should be trough its causing of $D_{i}$.
\item \textbf{First Stage or Relevance:} $\mathbb{E} \left[ D_{i1} - D_{i0} \right] \neq 0$. That is, $D_{i}$ is not a trivial function of the instrument. 
\item \textbf{Uniformity (poorly referred to as Monotonicity):} Either $D_{i1} \geq D_{i0}$ or $D_{i1} \leq D_{i0}$. That is, the instrument affects everyone in the same direction.
\end{itemize}

\item Show that the ATT and the LATE are not the same in general and explain why.\\ 
\noindent \textbf{Answer:} This also comes straight from the notes. It suffices to note that LATE is the average treatment effect on the compliers. In general, $\text{ATT}  \neq \text{LATE}$ because, ``always takers'' are treated $D_{i} = 1$ and LATE does not speak to them. In the absence of always and never takers, $\text{ATT}  = \text{LATE}$. If random assignment holds, however, it is also the case that $\text{ATT} = \text{ATE}$. That answers the next question! 
\item When is the LATE equal to the ATE? Provide a formal argument.\\ 
\noindent \textbf{Answer:} Done. 
\end{enumerate}

\noindent \textbf{Problem 5. Residual Instrumental Variables.} Consider an extension of the model in \textbf{Problem 1}, where the researcher aims to control for a vector of observed characteristics $\bm{x}_{i}$. 

\begin{enumerate} 
\item Why would a researcher aim to control for a vector of observed characteristics if the assumptions of the instrumental-variable setting hold?\\ 
\noindent \textbf{Answer:} Even when my full name is Jorge Residual Garc\'{i}a, there is no reason to control if the assumptions hold. In fact, you can see what the consequences of controlling could be by noting that $\beta^{\text{IV}}$ is the combination of simple expressions obtained from two regressions: the reduced-form and the first-stage regressions. It is not straightforward to see how those are combined once controls are accounted for.  
\item Is there another scenario where the researcher should consider a vector of observed characteristics?\\ 
\noindent \textbf{Answer:} Researchers consider controls when the assumptions of the instrumental-variable setting are more likely to hold, conditionally on $\bm{x}_i$. One example of that is in \textbf{Problem 3} where the lotteries are assigned by categorical bins and the bins are based on observed characteristics. 
\item Adapt and state the instrumental-variable setting assumption to consider the vector $\bm{x}_{i}$.\\
\noindent \textbf{Answer:} This requires to reconsider the assumptions conditional on $\bm{x}_i$. Although conceptually distinct, I compress random assignment and exclusion into exogeneity. I also make explicit the focus on binary treatment and instrument. 
\begin{itemize}
\item \textbf{Exogeneity:} $y_{iz}^d, D_{iz}  \independent z_{i} | \bm{x}_i $, for any values $d,z$. That is, the assignment of $z_{i}$ is independent of the realizations of the outcome or treatment take up.
\item \textbf{First Stage or Relevance:} $\mathbb{E} \left[ D_{i1} - D_{i0} | \bm{x}_i \right] \neq 0$. Equivalently,\\ 
$\Pr \left( D_{i} = 1 | z_{i} = 1, \bm{x}_i \right) \neq \Pr \left( D_{i} = 1 | z_{i} = 0, \bm{x}_i \right)$. That is, $D_{i}$ is not a trivial function of the instrument. 
\item \textbf{Uniformity (poorly referred to as Monotonicity):} Either $D_{i1} \geq D_{i0} | \bm{x}_i$ or $D_{i1} \leq D_{i0} | \bm{x}_i$. Equivalently,
$\Pr \left( D_{i1} \geq D_{i0} | \bm{x}_i \right) = 1$ or $\Pr \left( D_{i1} \leq D_{i0} | \bm{x}_i \right) = 1$. That is, the instrument affects everyone in the same direction with probability 1. 
\end{itemize}
\noindent Additionally, a \textbf{support or overlap} condition for the instrument and the observed characteristics is required: $\Pr \left( z_i | \bm{x}_i \right) \in \left( 0, 1 \right)$. 
\noindent Intuitively, this requires the standard assumptions to hold residually. 
\item Derive an expression for the $\text{LATE} \left( x \right) $.\\ 
\noindent \textbf{Answer:} It follows from the definition of LATE that 
\begin{equation}
\text{LATE} \left( x \right) := \mathbb{E} \left[ y_{i1}^{D_{i1}} - y_{i0}^{D_{i0}} | \text{compliers}, \bm{x}_i = x \right].
\end{equation}
\noindent What is more important is to show that the parameter is identified for $\bm{x}_i = x$. By exogeneity, 
\begin{equation}
\mathbb{E} \left[ y_{i}  | z_{i} = 1 , \bm{x}_i = x \right] - \mathbb{E} \left[ y_{i}  | z_{i} = 0 , \bm{x}_i = x \right] = \mathbb{E} \left[ y_{i1}^{D_{i1}} - y_{i0}^{D_{i0}} | \bm{x}_i = x \right].
\end{equation}

\noindent Uniformity and rules out defiers. Thus, 
\begin{equation}
\mathbb{E} \left[ y_{i1}^{D_{i1}} - y_{i0}^{D_{i0}} | \bm{x}_i = x \right] =  \mathbb{E} \left[ y_{i1}^{D_{i1}} - y_{i0}^{D_{i0}} | \bm{x}_i = x \right] \cdot \Pr \left( D_{i1} = 1, D_{i0} = 0 | \bm{x}_i = x \right), 
\end{equation}

\noindent which we can write as 
\begin{equation}
\frac{\mathbb{E} \left[ y_{i1}^{D_{i1}} - y_{i0}^{D_{i0}} | \bm{x}_i = x \right]}{\Pr \left( D_{i1} = 1, D_{i0} = 0 | \bm{x}_i = x \right)} =  \mathbb{E} \left[ y_{i1}^{D_{i1}} - y_{i0}^{D_{i0}} | \bm{x}_i = x, \text{compliers} \right] 
\end{equation}

\noindent by first stage. Overlap is also required. To see that note that $\Pr \left( D_{i1} = 1, D_{i0} = 0 | \bm{x}_i = x \right) = \mathbb{E} \left[ D_{i1} | z_{i} = 1,  \bm{x}_i = x \right] - \mathbb{E} \left[ D_{i1} | z_{i} = 0,  \bm{x}_i = x \right]$. If overlap did not hold, then it would not be possible to decide y the conditional probability of complying. Then
\begin{eqnarray}
\mathbb{E} \left[ y_{i1}^{D_{i1}} - y_{i0}^{D_{i0}} | \bm{x}_i = x, \text{compliers} \right] &=& \frac{\mathbb{E} \left[ y_{i}  | z_{i} = 1 , \bm{x}_i = x \right] - \mathbb{E} \left[ y_{i}  | z_{i} = 0 , \bm{x}_i = x \right]}{ \mathbb{E} \left[ D_{i1} | z_{i} = 1,  \bm{x}_i = x \right] - \mathbb{E} \left[ D_{i1} | z_{i} = 0,  \bm{x}_i = x \right] }.
\end{eqnarray}

\item Show and interpret the following 
\begin{eqnarray}
\text{LATE} &=& \mathbb{E} \left[ y_{i1} - y_{i0} | \text{compliers} \right] \nonumber \\ 
                   &=& \mathbb{E} \left[ \frac{ \text{LATE} \left( \bm{x_i} \right) \cdot  \Pr \left( \text{compliers} | \bm{x}_{i} \right) } {  \Pr \left( \text{compliers} \right) } \right]. 
\end{eqnarray}

\noindent Write 
\begin{eqnarray}
\mathbb{E} \left[ \frac{ \text{LATE} \left( \bm{x}_i \right) \cdot  \Pr \left( \text{compliers} | \bm{x}_{i} \right) } {  \Pr \left( \text{compliers} \right) } \right] &=& 
\frac{1}{\Pr \left( \text{compliers} \right)} \nonumber \\ &\cdot& \mathbb{E} \left[ \frac{\mathbb{E} \left[ y_{i}  | z_{i} = 1 , \bm{x}_i \right] - \mathbb{E} \left[ y_{i}  | z_{i} = 0 , \bm{x}_i \right] \cdot \Pr \left( \text{compliers} | \bm{x}_{i} \right)}{ \mathbb{E} \left[ D_{i1} | z_{i} = 1,  \bm{x}_i \right] - \mathbb{E} \left[ D_{i1} | z_{i} = 0,  \bm{x}_i \right] } \right] \nonumber \\ 
&=& \mathbb{E} \left[ \frac{\mathbb{E} \left[ y_{i}  | z_{i} = 1 , \bm{x}_i \right] - \mathbb{E} \left[ y_{i}  | z_{i} = 0 , \bm{x}_i \right]}{  \Pr \left( \text{compliers}  \right)  } \right] \nonumber \\ &=& \text{LATE}, 
\end{eqnarray}
\noindent where the first equality followsfrom the definition of $\text{LATE} \left( x \right)$ and the third from the definition of $\text{LATE}$. 

\item Assume that $\bm{x}_{i}$ is composed of a set of (binary) indicators and that the instrument, $z_{i}$, is binary. Note that the excluded instruments are now $z_{i}$ and all of the interactions of $\bm{x}_{i}$ with $z_{i}$. Think of a fully-saturated model. Show and interpret the following
\begin{eqnarray} 
\beta^{\text{TSLS}} &=& \mathbb{E} \left[ \text{LATE} \left( \bm{x}_{i} \right)   \frac{\var \left(  \Pr \left(  D_{i} = 1 | z_{i}, \bm{x}_{i} | \bm{x}_{i} \right)   \right)}{ \mathbb{E} \left[ \var \left(  \Pr \left(  D_{i} = 1 | z_{i}, \bm{x}_{i} | \bm{x}_{i} \right)   \right) \right] }  \right]. 
\end{eqnarray}
\end{enumerate}

\bibliographystyle{chicago}
\bibliography{bib}

\end{document}