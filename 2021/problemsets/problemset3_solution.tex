%Input preamble
\input{preamble}
\let\counterwithout\relax
\let\counterwithin\relax
\definecolor{maroon}{HTML}{4B0082}

\begin{document}

\noindent \textbf{900-03: Econometrics II: Advanced Regression Analysis and Empirical Design}\\
\noindent Instructor: Jorge Luis Garc√≠a \\
e-mail: jlgarci@clemson.edu\\

\noindent \textbf{Problem Set 3.}\\

\noindent \textbf{Submission instructions for empirical part:} Push the code for each empirical problem separately with the name problem``numberofproblem"\_``yourlastname".do. Do so to a folder in the repository called problemset3 (you need to create that folder in your local). \textit{Do not e-mail me your code}.  \textbf{Submission instructions for theory part:} Place a .pdf with your answer in the Box folder econ900/econ900-03/problemset3. \textit{Do not e-mail your pdf}.\\

\noindent \textbf{Due date:} 11/14/2020 at 8:00 p.m.\\

\noindent \textbf{Submission instructions.} The problem set is due on 9/24 at 14:00. Please submit the problem set as discussed in the fourth session of class.\\

\noindent \textbf{Problem 1. Matching, Theory.} Let $y_{i}^d$ be the outcome of individual $i$ when fixed to treatment ($D_{i} = 1$) or control $(D_{i} = 0)$. Let $y_{i}$ be the observed outcome and $D_{i}$ a treatment indicator. 

\begin{enumerate}
\item What does the notation mean? Specifically, what is the difference between the upper and lower cases of ``d.''\\
\noindent \textbf{Answer:} $y_{i}^d$ is the outcome of individual $i$ when she is fixed to either $D_{i} = 0$ or $D_{i} = 0$. Lowercase $d$ is the realization of the random variable $D_{i}$. $D_{i}$ indicates the outcome chosen by the individual.\\

\item What is the difference between fixing and conditioning?\\
\noindent \textbf{Answer:} Fixing is the conceptual exercise of assessing an outcome in a state of nature. Conditioning is the statistical exercise of summarizing a variable in a state of nature.\\ 

\item When, if ever, would you not care about this difference? \\
\noindent \textbf{Answer:} When $D_{i}$ is randomized, there is no difference between fixing and conditioning. Randomization fixes individuals to a state. Conditioning the variable will provide the summary of the variable in the state to which it is fixed by the randomization protocol.\\

\item What are the standard matching assumptions? Use $\bm{x}_{i}$ to denote the vector of observed characteristics.\\
\noindent \textbf{Answer:} The counterfactual outcomes are independent of the treatment choice, conditional on the observed characteristics. That is, $\left( y_{i}^1, y_{i}^0 \right) \independent D_{i} | \bm{x}_{i}$.\\

\item Explain these assumptions.\\
\noindent \textbf{Answer:} Once $\bm{x}_{i}$ is conditioned on, the choice of treatment, $D_{i}$, is as good as random (an assumption in which you better have the best $\bm{x}_{i}$ if you want to survive Prof. Simon's scrutiny).\\
\item Given these assumptions, propose an estimator for $\mathbb{E} \left[ y_{i}^1 - y_{i}^0 \right]$.\\
\noindent \textbf{Answer:} The counterfactual outcomes could be expressed as $y_{i} = D_{i}  y_{i}^1 + \left( 1 - D_{i} \right) y_{i}^0$. Then, note that 

\begin{eqnarray}
\text{ATE} &:=&  \mathbb{E} \left[ y_{i}^1 - y_{i}^0 \right] \nonumber \\
&=& \mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[ y_{i}  | D_{i} = 1 , \bm{x}_{i} \right] -   \mathbb{E} \left[ y_{i}  | D_{i} = 0 , \bm{x}_{i} \right]  \right] \nonumber \\
&=& \mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[ y_{i}  | \bm{x}_{i} \right] \right] \nonumber \\
 &=& \beta D_{i} + \bm{\gamma} \cdot \bm{x}_{i}, 
\end{eqnarray}

\noindent where $\bm{\gamma}$ is a coefficient vector. The last equality follows from a parametric assumption that we usually make in linear models: The conditional expectation is the same for each value of $\bm{x}_{i}$. An estimator for $\text{ATE}$ is the OLS estimator of $\beta$.

\item Propose an inference procedure.\\
\noindent \textbf{Answer:} Any inference procedure for OLS would be valid.\\
\item Show that the matching assumptions imply that $D_{i}$ is independent of the potential outcomes conditional on $\Pr \left[ D_{i} = 1 | \bm{x}_{i} \right]$ (propensity score, henceforth). \\
\noindent \textbf{Answer:} See Theorem 3.3.1 and its proof in \citet{angrist2008mostly} and read the intuition.\\

\item Interpret the result of your proof.\\
\noindent \textbf{Answer:} The propensity score suffices as a summary of  $\bm{x}_{i}$ for purposes of conditioning.\\

\item Propose an estimator of $\mathbb{E} \left[ y_{i}^1 - y_{i}^0 \right]$ that uses the propensity score.\\
\noindent \textbf{Answer:} Straight from the notes we have that the propensity-score matching estimator of the ATE is 

\begin{eqnarray}
\text{ATE} &:=&  \mathbb{E} \left[ y_{i}^1 - y_{i}^0 \right] \nonumber \\
&=& \mathbb{E}_{\bm{x}_{i}} \left[ \frac{D_{i} - p \left(  \bm{x}_{i} \right) } { p \left(  \bm{x}_{i} \right) \left( 1 - p \left(  \bm{x}_{i} \right) \right)  } | \bm{x}_{i} \right], 
\end{eqnarray}
\noindent where $p \left(  \bm{x}_{i} \right) := \Pr \left[ D_{i} = 1 | \bm{x}_{i} \right]$.

\item Propose an inference procedure.\\
\noindent \textbf{Answer:} The inference procedure needs to take into account the sampling variation involved in computing both $p \left(  \bm{x}_{i} \right) $ and the ATE itself. The bootstrap is a good option. 

\item A speaker in the seminar has a model like that in Equation~\eqref{eq:model} and calls his empirical strategy ``matching.'' Professor Simon calls him out and tells him that he is ``simply running OLS.'' Map the notation in this problem into Equation~\eqref{eq:model}  to defend the speaker from Professor Simon. Is the speaker ``overselling'' his empirical strategy or is Professor Simon in a mood and that is why he is calling the speaker out?\\
\noindent \textbf{Answer:} Let $\bm{x}_{i1}:= D_{i}$, $\bm{\beta}_1 =  \beta$,  $\bm{x}_{i2}:= \bm{x}_{i}$, $\bm{\beta}_2 =  \gamma$. This maps the matching notation to a linear regression and OLS becomes an evident estimation method. Thus, the speaker is right to call his strategy matching. Prof. Simon was in a mood!
\end{enumerate}

\noindent \textbf{Problem 2. Matching, Practice.} Your outcome of interest is test scores and your policy binary variable of interest is homework completion. You have data on test scores in the year that you are analyzing, homework completion, number of class absences, gender, age, test scores in the previous year.

\begin{enumerate} 
\item What is the parameter of interest for abolishing homework?\\
\noindent \textbf{Answer:} The parameter of interest is the average treatment on the treated. This parameter allows us to understand the effect on homework on the people currently ``treated'' by it.\\

\item What is the parameter of interest for making homework mandatory? \\
\noindent \textbf{Answer:} The parameter of interest is the average treatment on the untreated. This parameter allows us to understand the effect on homework on the people currently ``untreated'' by it.\\

\item What is the parameter of interest for comparing the effect of an extra hour of schooling relative to doing homework?  \\
\noindent \textbf{Answer:} The average effect of an extra hour of schooling, keeping fixed the level of homework currently assigned.\\

\item Suppose that you can only use matching. What fundamental assumption would allow you to study how would the students who currently do not do homework do if they did the homework?  \\
\noindent \textbf{Answer:} The conditional independence assumption.\\

\item What is the parameter of interest?\\
\noindent \textbf{Answer:} For the same reason as in 2., the average treatment on the untreated.\\

\item What are the counterfactual outcomes of interest? \\
\noindent \textbf{Answer:} The test scores fixed to states where homework is assigned and not assigned, \textit{conditional} on being untreated. 

\item Provide an OLS estimator of the parameter of interest. \\
\noindent \textbf{Answer:} Note that 

\begin{eqnarray} 
\text{ATU} &:=& \mathbb{E} \left[ y_{i}^1 - y_{i}^0 | D = 0 \right] \nonumber \\
                 &=& \mathbb{E}_{\bm{x}_i} \left[ \mathbb{E} \left[ y_{i} | D = 1 , \bm{x}_{i} \right]  | D = 0 \right] - \mathbb{E} \left[  y_{i} | D = 0 \right],
\end{eqnarray}

\noindent for which there is no direct OLS estimator if your sample only contains $D = 0$. A weighting or matching procedure is required. 

\item Provide examples of violations to the fundamental assumption biasing the estimate upwards.\\
\noindent \textbf{Answer:} Once an estimate is available, the CIA is violated and the ATU is biased upwards if less able students decide not to do homework: $\mathbb{E} \left[  y_{i} | D = 0 \right]$ would be biased downwards.
\item Provide examples of violations to the fundamental assumption biasing the estimate downwards.\\
\noindent \textbf{Answer:} Follows directly from the previous exercise.
\end{enumerate}

\noindent \textbf{Problem 3. TFU is not only for Price Theory.}

\begin{enumerate} 
\item The Conditional Independence Assumption is untestable.\\ 
\textbf{Answer:} True. The assumption states that $\left( Y_{i}^1, Y_{i}^0 \right) \independent D_{i} | \bm{x}_{i}$, using standard notation. It is impossible to observe both $Y_{i}^1$ and $Y_{i}^0$ and, therefore, it is impossible to test the assumption.

\item When solving missing data problems, hot-decking and cold-decking are based on the same assumptions as IPW.\\ 
\textbf{Answer:} True. Because both hot-decking and cold-decking are methods imputing data based on ``conditionally equivalent'' people, they are based on the assumption that, once $\bm{x}_{i}$ is conditioned on, missing data appear at random. If we were to weight people based on their propensity to be missing, we would also assume that data are missing conditionally at random.\\

\item If you have a randomized control trial and you want to obtain the ATE, you can either (i) use an estimator based on means; or (ii) regress the outcomes of interest on a treatment indicator. It is never correct to include an intercept in the regression of option (ii).\\ 
\textbf{Answer:} False. Including an intercept changes the interpretation of the estimated coefficients but not the overall information obtained from a regression.\\

\item If you have a randomized control trial and you want to obtain the ATE, you can either (i) use an estimator based on means; or (ii) regress the outcomes of interest on a treatment indicator. It is never incorrect to include controls in the regression of option (ii).\\ 
\textbf{Answer:} False. Treatment assignment is randomized in this context. Thus, it is independent of the controls. Thus, the estimate of the ATE is not affected by including controls (additionally, it lowers the standard errors because it captures variation based on observed characteristics).

Including an intercept changes the interpretation of the estimated coefficients but not the overall information obtained from a regression.\\

\item The estimates obtained from IPW weighted regressions are numerically equivalent to estimates obtained from unweighted OLS regressions.\\ 
\textbf{Answer:} False. OLS is a particular case of IPW where the weights are equal to one.\\

\item Data from experiments are always better for answering policy questions if compared to observational data.\\ 
\textbf{Answer:} False. The origin of the data itself does not provide an answer to a policy question. Data are combined with methodology to answer policy questions. \\
\end{enumerate} 

\noindent \textbf{Problem 5. More IPW.} Suppose that the data-generating process takes the form: 

\begin{eqnarray} 
y_{i}^0 &=& 2 + 2 \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i}^0 \nonumber \\ 
y_{i}^1 &=& 3 + 2 \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i}^1, \label{eq:dgp}
\end{eqnarray} 

\noindent with $\mathbb{E}\left[  \varepsilon_{i}^0 \right] = \mathbb{E}\left[  \varepsilon_{i}^1 \right] = 0$ and where the standard notation applies. 

\begin{enumerate} 
\item State the conditional independence assumption and write its implications with respect to the distributions of the counterfactual outcomes and unobserved components. Assume that the CIA holds henceforth.\\
\noindent \textbf{Answer:} The conditional independence assumption is: $\left( Y_{i}^0, Y_{i}^0 \right) \independent  D_i | \bm{x}_i $. This implies that  $\left( \varepsilon_{i}^0, \varepsilon_{i}^0 \right) \independent  D_i | \bm{x}_i $. 

\item Calculate the ATE?\\
\noindent \textbf{Answer:} Let $\bm{x}_{i} := \left[  \bm{x}_{i1}, \bm{x}_{i2} \right]$.  
\begin{eqnarray}
\text{ATE} &:=& \mathbb{E} \left[ y_{i}^1 - y_{i}^0  \right] \nonumber \\
&=&   \mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[ y_{i}^1 - y_{i}^0 | \bm{x}_{i} \right] \right] \nonumber \\
&=&  \mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[  \left( 3 + 2 \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i}^1 \right) -  \left(2 + 2 \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i}^0\right) | \bm{x}_{i} \right] \right] \nonumber \\ 
&=&  1 + \mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[  \varepsilon_{i}^1  | \bm{x}_{i} \right] \right] - \mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[  \varepsilon_{i}^ 0 | \bm{x}_{i} \right] \right] \nonumber \\
&=&  1 +\mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[  0  | \bm{x}_{i} \right] \right] - \mathbb{E}_{\bm{x}_{i}} \left[ \mathbb{E} \left[ 0 | \bm{x}_{i} \right] \right] \nonumber \\ 
&=& 1
\end{eqnarray}

\item Is the ATE that you wrote down an estimate or a true value? \\
\noindent \textbf{Answer:} A true value because because Equation~\eqref{eq:dgp} represents the true generating process.
\item The joint distribution of treatment take up and observed characteristics is

\begin{center}
\begin{tabular}{ccccccc} \toprule
& \multicolumn{3}{c}{\underline{$D_{i} = 1$}} &  \multicolumn{3}{c}{\underline{$D_{i} = 0$}} \\
& $\bm{x}_{i1} = 0$ & $\bm{x}_{i1} = 1$ & $\bm{x}_{i1} = 2$ & $\bm{x}_{i1} = 0$ & $\bm{x}_{i1} = 1$ & $\bm{x}_{i1} = 2$ \\
$\bm{x}_{i2} = 0$ & .1 & .05 & .1 & .1 & .1& .05 \\ 
$\bm{x}_{i2} = 1$ & .1 & .05 & .1 & .1 & .05 & 1 \\  \bottomrule
\end{tabular}
\end{center}

\noindent A researcher is interested in estimating the ATE and writes down the following linear model 

\begin{equation}
y_i = \beta_0 + \beta_1 \bm{x}_{i1} + \beta_2 \bm{x}_{i2} + \beta_3 D_{i} + \eta_{i}
\end{equation}

\begin{enumerate} 
\item Show (empirically) that the OLS estimator of $\beta_3$ that he is setting up is an inconsistent estimator of the ATE.\\
\noindent \textbf{Answer:} First write 

\begin{eqnarray}
y_{i} := D_{i} \cdot y_{i}^1 + \left( 1 - D_{i} \right) \cdot y_{i}^0 \nonumber \\
         = \gamma_0 + \gamma_1 \cdot  \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i}, 
\end{eqnarray}

\noindent where, empirically, $\gamma_0:= 1, \gamma_1 = 2$, and $\varepsilon_{i}:= \varepsilon_{i}^1 - \varepsilon_{i}^0$.
\begin{eqnarray} 
\plim \left( \hat{\beta}_3^{\text{OLS}} \right) &=& \frac{ \cov \left( D_{i}, y_{i} \right) }{ \var \left( D_{i} \right) } \nonumber \\
&=&  \frac{ \cov \left( D_{i},   \gamma_0 + \gamma_1 \cdot  \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i} \right) }{ \var \left( D_{i} \right) } \nonumber \\
&=& 1 + \gamma_1 + \frac{ \cov \left( D_{i}, \bm{x}_{i1} \cdot \bm{x}_{i2} \right) }{ \var \left( D_{i} \right) } \nonumber \\
&\neq & 1 \nonumber \\ 
&=& \text{ATE}
\end{eqnarray}
Note that $\Pr \left( D_{i} = 1 | \bm{x}_{ik} \right) =  \Pr \left( D_{i} = 0 | \bm{x}_{ik} \right)$ for $k = 1,2$ so that $\bm{x}_{ik} \independent D_{i}$. However, $\bm{x}_{i1} \cdot \bm{x}_{i2}$ is not independent of $D_{i}$. Thus the bias.

\item Suppose that the researcher estimates the propensity score using the following linear model: 

\begin{equation}
D_i = \alpha_0 + \alpha_1 \bm{x}_{i1} + \alpha_2 \bm{x}_{i2} + \upsilon_{i} 
\end{equation}

\noindent to then implement an IPW scheme. Show (empirically) that this model produces an inconsistent estimate of the ATE.\\
\noindent \textbf{Answer:} There is not even need to show anything here after the last answer. The IPW scheme of the researcher aims to enforce something that is already empirically true: $\bm{x}_{ik} \independent D_{i}$ for $k = 1,2$. Selection based on $\bm{x}_{i1} \cdot \bm{x}_{i2}$ is the problem and their scheme does not solve it.

\end{enumerate}
\item Interpret the exercise and answer the following: Would fully saturating the model have helped the researcher?\\
\noindent \textbf{Answer:} The exercise establishes that model misspecification could be ``as bad'' as violating conditional independence. In this case, a simple non-linearity between $\bm{x}_{i1}$ and $\bm{x}_{i2}$ generates inconsistency. Fully-saturating the model could have solved this issue. Suppose that $\bm{x}_{i1}$ and $\bm{x}_{i2}$ were dummy variables. A fully saturated specification would have been 

\begin{eqnarray} 
y_{i}^0 &=& \gamma_0^0 + \gamma_1^0 \cdot \bm{x}_{i1} + \gamma_2^0 \bm{x}_{i2} + \gamma_3^0 \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i}^0 \nonumber \\ 
y_{i}^1 &=& \gamma_0^1 + \gamma_1^1 \cdot \bm{x}_{i1} + \gamma_2^1 \bm{x}_{i2} + \gamma_3^1 \cdot \bm{x}_{i1} \cdot \bm{x}_{i2} + \varepsilon_{i}^1, \label{eq:dgp2}
\end{eqnarray} 

\noindent As the sample size grew large, this would have generated the probability limits to be $ \gamma_1^0 =  \gamma_1^1 = 0$. $\beta_3$, the coefficient on a treatment indicator, would have given $\gamma_0^1 - \gamma_0^0$ (the ATE). The coefficient on $\bm{x}_{i1} \cdot \bm{x}_{i2}$ would have been $2$.

\end{enumerate} 

\bibliographystyle{chicago}
\bibliography{bib}

\end{document}